<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Barry Quinn">

<title>Advanced Financial Data Analytics - 6&nbsp; Bayesian Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ml.html" rel="next">
<link href="./time_series.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./primer.html">Core Concepts</a></li><li class="breadcrumb-item"><a href="./bayesian_methods.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanced Financial Data Analytics</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Core Concepts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistics and Probability Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Toolkit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./returns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Understanding Financial Data and Asset Returns</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time_series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Financial times series econometrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian_methods.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Machine Learning in Finance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Advanced Material</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multilevel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modeling Multilevel Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Causal inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./colliderbias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Causal Salad, Endogeneity and collider bias</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Chapter solutions</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#bayesian-thinking-for-finance" id="toc-bayesian-thinking-for-finance" class="nav-link active" data-scroll-target="#bayesian-thinking-for-finance"><span class="header-section-number">6.1</span> Bayesian thinking for finance</a>
  <ul class="collapse">
  <li><a href="#the-essence-of-bayesian-thinking" id="toc-the-essence-of-bayesian-thinking" class="nav-link" data-scroll-target="#the-essence-of-bayesian-thinking"><span class="header-section-number">6.1.1</span> The Essence of Bayesian Thinking</a></li>
  <li><a href="#practical-implications-in-finance" id="toc-practical-implications-in-finance" class="nav-link" data-scroll-target="#practical-implications-in-finance"><span class="header-section-number">6.1.2</span> Practical Implications in Finance</a></li>
  </ul></li>
  <li><a href="#basics-of-bayesian-statistics" id="toc-basics-of-bayesian-statistics" class="nav-link" data-scroll-target="#basics-of-bayesian-statistics"><span class="header-section-number">6.2</span> Basics of Bayesian Statistics</a>
  <ul class="collapse">
  <li><a href="#introduction-to-bayesian-statistics" id="toc-introduction-to-bayesian-statistics" class="nav-link" data-scroll-target="#introduction-to-bayesian-statistics"><span class="header-section-number">6.2.1</span> Introduction to Bayesian Statistics</a></li>
  <li><a href="#example-bayesian-inference-in-investment-decision-making" id="toc-example-bayesian-inference-in-investment-decision-making" class="nav-link" data-scroll-target="#example-bayesian-inference-in-investment-decision-making"><span class="header-section-number">6.2.2</span> Example: Bayesian Inference in Investment Decision-Making</a></li>
  </ul></li>
  <li><a href="#bayesian-inference-for-univariate-normal-models-expanded" id="toc-bayesian-inference-for-univariate-normal-models-expanded" class="nav-link" data-scroll-target="#bayesian-inference-for-univariate-normal-models-expanded"><span class="header-section-number">6.3</span> Bayesian Inference for Univariate Normal Models (Expanded)</a>
  <ul class="collapse">
  <li><a href="#deriving-the-posterior-density-analytically" id="toc-deriving-the-posterior-density-analytically" class="nav-link" data-scroll-target="#deriving-the-posterior-density-analytically"><span class="header-section-number">6.3.1</span> Deriving the Posterior Density Analytically</a></li>
  <li><a href="#expressions-for-the-normal-and-inverse-gamma-distributions" id="toc-expressions-for-the-normal-and-inverse-gamma-distributions" class="nav-link" data-scroll-target="#expressions-for-the-normal-and-inverse-gamma-distributions"><span class="header-section-number">6.3.2</span> Expressions for the Normal and Inverse-Gamma Distributions</a></li>
  <li><a href="#graphical-representation-of-densities" id="toc-graphical-representation-of-densities" class="nav-link" data-scroll-target="#graphical-representation-of-densities"><span class="header-section-number">6.3.3</span> Graphical Representation of Densities</a></li>
  <li><a href="#calculating-moments" id="toc-calculating-moments" class="nav-link" data-scroll-target="#calculating-moments"><span class="header-section-number">6.3.4</span> Calculating Moments</a></li>
  <li><a href="#sampling-from-the-joint-posterior-distribution" id="toc-sampling-from-the-joint-posterior-distribution" class="nav-link" data-scroll-target="#sampling-from-the-joint-posterior-distribution"><span class="header-section-number">6.3.5</span> Sampling from the Joint Posterior Distribution</a></li>
  <li><a href="#bayesian-credible-intervals" id="toc-bayesian-credible-intervals" class="nav-link" data-scroll-target="#bayesian-credible-intervals"><span class="header-section-number">6.3.6</span> Bayesian Credible Intervals</a></li>
  </ul></li>
  <li><a href="#motivation-for-hierarchical-models" id="toc-motivation-for-hierarchical-models" class="nav-link" data-scroll-target="#motivation-for-hierarchical-models"><span class="header-section-number">6.4</span> Motivation for Hierarchical Models</a>
  <ul class="collapse">
  <li><a href="#examples-in-r" id="toc-examples-in-r" class="nav-link" data-scroll-target="#examples-in-r"><span class="header-section-number">6.4.1</span> Examples in R</a></li>
  <li><a href="#traditional-econometrics-versus-bayesian-hierarchical-models" id="toc-traditional-econometrics-versus-bayesian-hierarchical-models" class="nav-link" data-scroll-target="#traditional-econometrics-versus-bayesian-hierarchical-models"><span class="header-section-number">6.4.2</span> Traditional econometrics versus Bayesian hierarchical models</a></li>
  <li><a href="#r-example-frequentist-versus-bayesian-approach" id="toc-r-example-frequentist-versus-bayesian-approach" class="nav-link" data-scroll-target="#r-example-frequentist-versus-bayesian-approach"><span class="header-section-number">6.4.3</span> R Example: Frequentist Versus Bayesian Approach</a></li>
  </ul></li>
  <li><a href="#mcmc-methods" id="toc-mcmc-methods" class="nav-link" data-scroll-target="#mcmc-methods"><span class="header-section-number">6.5</span> MCMC Methods</a>
  <ul class="collapse">
  <li><a href="#example-in-r-simple-random-walk-metropolis-hastings-sampler" id="toc-example-in-r-simple-random-walk-metropolis-hastings-sampler" class="nav-link" data-scroll-target="#example-in-r-simple-random-walk-metropolis-hastings-sampler"><span class="header-section-number">6.5.1</span> Example in R: Simple random walk Metropolis-Hastings sampler</a></li>
  </ul></li>
  <li><a href="#bayesian-approaches-to-model-financial-data" id="toc-bayesian-approaches-to-model-financial-data" class="nav-link" data-scroll-target="#bayesian-approaches-to-model-financial-data"><span class="header-section-number">6.6</span> Bayesian Approaches to Model Financial Data</a>
  <ul class="collapse">
  <li><a href="#ar1-example-in-r" id="toc-ar1-example-in-r" class="nav-link" data-scroll-target="#ar1-example-in-r"><span class="header-section-number">6.6.1</span> AR(1) Example in R</a></li>
  <li><a href="#arma11-example-in-r" id="toc-arma11-example-in-r" class="nav-link" data-scroll-target="#arma11-example-in-r"><span class="header-section-number">6.6.2</span> ARMA(1,1) Example in R</a></li>
  <li><a href="#volatility-models-ex" id="toc-volatility-models-ex" class="nav-link" data-scroll-target="#volatility-models-ex"><span class="header-section-number">6.6.3</span> Volatility Models EX</a></li>
  </ul></li>
  <li><a href="#common-misconceptions-of-bayesian-econometrics-by-traditional-frequentist-econometricians" id="toc-common-misconceptions-of-bayesian-econometrics-by-traditional-frequentist-econometricians" class="nav-link" data-scroll-target="#common-misconceptions-of-bayesian-econometrics-by-traditional-frequentist-econometricians"><span class="header-section-number">6.7</span> Common Misconceptions of Bayesian Econometrics by Traditional Frequentist Econometricians</a>
  <ul class="collapse">
  <li><a href="#further-reading-for-bayesian-methods-in-finance" id="toc-further-reading-for-bayesian-methods-in-finance" class="nav-link" data-scroll-target="#further-reading-for-bayesian-methods-in-finance"><span class="header-section-number">6.7.1</span> Further Reading for Bayesian Methods in Finance</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Barry Quinn </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p><img src="images/logos/DALL·E 2024-01-18 16.32.37 - Create a professional logo for an advanced financial data analytics course. The design should be vibrant, with a medium level of detail, balancing com.png" class="img-fluid" style="float: left; margin-right: 10px;;width:50.0%"></p>
<section id="bayesian-methods-offer-a-powerful-alternative-to-traditional-statistical-analysis-in-the-world-of-finance.-these-methods-incorporate-prior-knowledge-and-update-beliefs-based-on-new-data-providing-a-dynamic-approach-to-financial-analysis.-weeks-3-and-4-of-the-course-will-delve-into-these-methods-and-their-practical-applications." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bayesian-methods-offer-a-powerful-alternative-to-traditional-statistical-analysis-in-the-world-of-finance.-these-methods-incorporate-prior-knowledge-and-update-beliefs-based-on-new-data-providing-a-dynamic-approach-to-financial-analysis.-weeks-3-and-4-of-the-course-will-delve-into-these-methods-and-their-practical-applications.">Bayesian methods offer a powerful alternative to traditional statistical analysis in the world of finance. These methods incorporate prior knowledge and update beliefs based on new data, providing a dynamic approach to financial analysis. Weeks 3 and 4 of the course will delve into these methods and their practical applications.</h4>
<p><br></p>
</section>
<section id="bayesian-thinking-for-finance" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="bayesian-thinking-for-finance"><span class="header-section-number">6.1</span> Bayesian thinking for finance</h2>
<p>Bayesian methods in finance represent a paradigm shift from traditional statistical methodologies, offering a unique approach to the interpretation of financial data. These methods, grounded in Bayesian thinking, integrate prior knowledge with observed data, providing a dynamic framework for financial analysis and decision-making.</p>
<section id="the-essence-of-bayesian-thinking" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="the-essence-of-bayesian-thinking"><span class="header-section-number">6.1.1</span> The Essence of Bayesian Thinking</h3>
<p>Bayesian thinking is characterised by its foundational belief in the integration of prior information with observed data. This approach contrasts with traditional frequentist methods, which solely rely on data without incorporating prior beliefs or information. The Bayesian perspective is rooted in the application of Bayes’ theorem, a fundamental principle that updates the probability estimate for a hypothesis as new evidence is presented.</p>
<section id="unconventional-yet-provocative" class="level4" data-number="6.1.1.1">
<h4 data-number="6.1.1.1" class="anchored" data-anchor-id="unconventional-yet-provocative"><span class="header-section-number">6.1.1.1</span> Unconventional Yet Provocative</h4>
<p>While Bayesian methods are not entirely new, they often present unconventional viewpoints that challenge the norms of traditional econometrics. These methods have been perceived as both thought-provoking and, occasionally, controversial among econometricians. Despite this, the role of Bayesian thinking in finance is increasingly recognized for its practicality and relevance, particularly in areas where frequentist methods have dominated.</p>
</section>
<section id="bridging-the-gap" class="level4" data-number="6.1.1.2">
<h4 data-number="6.1.1.2" class="anchored" data-anchor-id="bridging-the-gap"><span class="header-section-number">6.1.1.2</span> Bridging the Gap</h4>
<p>One of the key discussions in the application of Bayesian methods in finance revolves around areas where frequentist asymptotics have been dominant. Bayesian approaches offer an alternative that can be more practical and prevalent, especially in complex financial models where integrating prior knowledge and uncertainty can significantly enhance model robustness and inference quality.</p>
</section>
</section>
<section id="practical-implications-in-finance" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="practical-implications-in-finance"><span class="header-section-number">6.1.2</span> Practical Implications in Finance</h3>
<p>The implementation of Bayesian methods in financial econometrics has significant implications. These include more nuanced risk assessment, enhanced portfolio optimization strategies, and improved forecasting models that take into account both historical data and expert knowledge. Bayesian methods’ flexibility and adaptability make them particularly suitable for financial markets, which are often influenced by a myriad of known and unknown factors.</p>
<section id="towards-a-more-practical-approach" class="level4" data-number="6.1.2.1">
<h4 data-number="6.1.2.1" class="anchored" data-anchor-id="towards-a-more-practical-approach"><span class="header-section-number">6.1.2.1</span> Towards a More Practical Approach</h4>
<p>The shift towards Bayesian methods in finance is driven by the need for more practical and comprehensive tools in decision-making processes. The Bayesian framework’s ability to incorporate prior beliefs and continuously update these beliefs as new data becomes available aligns well with the dynamic nature of financial markets.</p>
<p>In summary, Bayesian methods bring a distinct and valuable perspective to financial data analysis. Their emphasis on integrating prior information with empirical data offers a more holistic approach to understanding and predicting financial market behaviors.</p>
</section>
</section>
</section>
<section id="basics-of-bayesian-statistics" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="basics-of-bayesian-statistics"><span class="header-section-number">6.2</span> Basics of Bayesian Statistics</h2>
<ul>
<li><strong>Overview</strong>: Bayesian statistics involves updating the probability for a hypothesis as more evidence or information becomes available. It contrasts with the frequentist approach by incorporating prior beliefs.</li>
<li><strong>Bayesian Inference</strong>: The process of deducing properties about a population or probability distribution from data using Bayes’ theorem.</li>
<li><strong>Prior, Likelihood, and Posterior</strong>: Key concepts in Bayesian analysis where the prior represents initial beliefs, the likelihood is the probability of the data under the model, and the posterior is the updated belief after considering the data.</li>
</ul>
<section id="introduction-to-bayesian-statistics" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="introduction-to-bayesian-statistics"><span class="header-section-number">6.2.1</span> Introduction to Bayesian Statistics</h3>
<p>In this section, we delve deeper into the fundamental concepts of Bayesian statistics, building on top of the brief introduction given earlier. We explain the main terminologies involved, along with graphical representations and calculations associated with them. This helps establish a strong foundation for further study of Bayesian methods in finance.</p>
<section id="terminologies-and-definitions" class="level4" data-number="6.2.1.1">
<h4 data-number="6.2.1.1" class="anchored" data-anchor-id="terminologies-and-definitions"><span class="header-section-number">6.2.1.1</span> Terminologies and Definitions</h4>
<ol type="1">
<li><p><strong>Probability</strong>: Probability is a numerical measure representing the chance or likelihood that a particular event occurs. Its value ranges from 0 (impossible) to 1 (certainty). Mathematically, it satisfies certain rules called Kolmogorov’s axioms. For discrete variables, <span class="math inline">\(p(X)\)</span> represents the summed probabilities over all possible outcomes of <span class="math inline">\(X\)</span>. Similarly, for continuous variables, <span class="math inline">\(p(X)\)</span> denotes the integral evaluated over all possible outcomes of <span class="math inline">\(X\)</span>, often expressed as a probability density function (PDF).</p></li>
<li><p><strong>Parameter</strong>: In statistics, parameters refer to unknown quantities characterizing a population. These could include population means, variances, proportions, correlation coefficients, and others. Our goal typically involves making informed statements about these parameters based on observed data from a sample drawn from the larger population.</p></li>
<li><p><strong>Statistic</strong>: Statistic refers to a quantity derived from sample data. Unlike parameters, statistics represent known values calculated directly from observed data. Common examples include sample means, medians, percentiles, correlations, and regression coefficients.</p></li>
<li><p><strong>Prior</strong>, <span class="math inline">\(\pi(\theta)\)</span>: Before observing any data, a prior belief regarding the likely range of plausible values for the parameter(s) (<span class="math inline">\(\theta\)</span>) of interest is specified in the form of a probability distribution, referred to as the prior distribution. This expresses prior knowledge, assumptions, or beliefs held before seeing any data. When little information exists, one opts for relatively uninformative priors to avoid biasing conclusions unduly. On the contrary, when substantial domain knowledge is available, highly informative priors can incorporate such expert judgments effectively.</p></li>
<li><p><strong>Likelihood</strong>, <span class="math inline">\(f(x \mid \theta)\)</span>: Given a set of fixed parameter values, likelihood quantifies the probability of obtaining the observed sample data (<span class="math inline">\(x\)</span>). In essence, it acts as a bridge connecting hypothesized parameter values with actual evidence contained in the data. By varying the parameter values, we derive corresponding likelihood values, revealing which combinations align best with the data at hand.</p></li>
<li><p><strong>Posterior</strong>, <span class="math inline">\(p(\theta \mid x)\)</span>, also denoted as <span class="math inline">\(\pi(\theta \mid x)\)</span>: Once the prior and likelihood have been defined, Bayes’ rule allows us to update our initial belief system (prior) with the newly acquired empirical information (likelihood), leading to the formation of a refined, updated belief encapsulated in the posterior distribution. Formally stated, the posterior captures the conditional distribution of parameters (<span class="math inline">\(\theta\)</span>), conditioned on the observed data (<span class="math inline">\(x\)</span>). Mathematically, Bayes’ rule states: <span class="math display">\[
\underbrace{p(\theta \mid x)}_{\text{{Posterior}}} = \frac{\overbrace{f(x \mid \theta)}^{\text{{Likelihood}}}\times \overbrace{\pi(\theta)}^{\text{{Prior}}}}{\int f(x \mid \theta)\cdot \pi(\theta)\mathrm{d}\theta}.
\]</span> Note that the denominator serves as a scaling constant ensuring proper normalization of the posterior distribution.</p></li>
<li><p><strong>Marginal likelihood</strong> or <strong>Evidence</strong>, <span class="math inline">\(f(x)\)</span>: Also referred to as the model evidence, marginal likelihood arises due to the need to integrate out nuisance parameters from the full joint distribution while computing the posterior distribution. Marginal likelihood plays a crucial role in comparing competing models since higher marginal likelihood implies better overall fit of the model to the data.</p></li>
<li><p><strong>Conjugacy</strong>: Conjugacy describes the property where the functional forms of the prior and posterior belong to the same parametric family of distributions. Such relationships simplify computations significantly, especially in cases where closed-form solutions exist. Many well-known pairs of conjugate distributions facilitate straightforward mathematical manipulations, thereby rendering analytical expressions feasible even without resorting to computationally intensive algorithms.</p></li>
</ol>
<p>Next, we discuss several aspects of prior distributions, explaining various ways to specify them and understand their impact on posterior inferences.</p>
</section>
<section id="specifying-prior-distributions" class="level4" data-number="6.2.1.2">
<h4 data-number="6.2.1.2" class="anchored" data-anchor-id="specifying-prior-distributions"><span class="header-section-number">6.2.1.2</span> Specifying Prior Distributions</h4>
<p>When choosing a prior distribution, multiple options exist depending on whether we possess substantive domain knowledge or merely vague hunches concerning the likely range of plausible values for the parameters. Accordingly, we categorize prior distributions into broad classes—informative and uninformative priors.</p>
<ol type="1">
<li><p><strong>Uninformative Priors</strong>: Often chosen when lacking sufficient prior knowledge about the parameters, uninformative priors aim to minimize influence on posterior inferences by assigning equal weight across wide swaths of potential parameter values. Some commonly employed choices include uniform distributions spanning large domains, Jeffreys priors, reference priors, or improper flat priors. However, extreme care must be taken while selecting uninformatively because seemingly innocuous decisions can still exert disproportional impacts on subsequent analyses. Moreover, misuse or misunderstanding of such priors may lead to flawed conclusions and biased inferences.</p></li>
<li><p><strong>Weakly Informative Priors</strong>: Alternatively, weakly informative priors strike a delicate balance between imparting minimal guidance and conveying subtle hints regarding reasonable bounds encompassing probable parameter values. Typically, these take the form of mildly peaked distributions exhibiting wider spread than conventional informative priors but narrower dispersion relative to uninformative alternatives. Prominent instances include Gaussian distributions centered around zero with moderately small variances, Laplace distributions concentrated near origin with modest scales, or half-Cauchy distributions truncated below zero having moderate scale factors. Although not strictly equivalent to uninformative priors, weakly informative counterparts generally yield similar qualitative patterns in posterior distributions while mitigating risks posed by arbitrarily assigned uninformative priors.</p></li>
<li><p><strong>Informative Priors</strong>: Based on ample prior information stemming from domain experts, historical records, previous studies, meta-analytic reviews, or elicitations, informative priors assume central roles in guiding posterior inferences towards desirable regions reflecting genuine underlying phenomena rather than mere artifacts resulting from poorly chosen priors. Ideally, such priors convey accurate representations of reality anchored firmly in reliable foundations backed by sound scientific reasoning and rigorous documentation. Popular choices include Gaussian distributions centered around sensible locations endowed with appropriate precisions, Bernoulli distributions manifesting believable success probabilities, Poisson distributions embodying realistic rate parameters, or Dirichlet distributions exemplified by meaningful mixture weights. Nevertheless, caution ought to be exercised when invoking strongly informative priors since excessive reliance on untested premises can potentially obscure valuable signals hidden within the data itself.</p></li>
</ol>
</section>
<section id="impact-of-prior-distributions" class="level4" data-number="6.2.1.3">
<h4 data-number="6.2.1.3" class="anchored" data-anchor-id="impact-of-prior-distributions"><span class="header-section-number">6.2.1.3</span> Impact of Prior Distributions</h4>
<p>As previously mentioned, the choice of prior distribution heavily influences subsequent inferences derived from posteriors. To gain intuition behind this phenomenon, consider the following aspects affecting prior sensitivity:</p>
<ol type="1">
<li><p><strong>Data Volume</strong>: As the volume of available data increases, the contribution of the prior diminishes considerably owing to overwhelming empirical evidence overshadowing initially espoused convictions embodied within the prior. Essentially, as more data become accessible, the posterior converges toward the maximum likelihood estimator, irrespective of the adopted prior. At extremes, this situation translates into asymptotic insensitivity wherein the ultimate choice of prior becomes inconsequential.</p></li>
<li><p><strong>Model Complexity</strong>: With increasing complexity introduced via sophisticated structural dependencies, intricate latent constructs, or nested hierarchies, the necessity for judicious prior selection amplifies accordingly. More elaborate architectures demand greater scrutiny vis-à-vis priors precisely because they harbor numerous interconnected components susceptible to being swayed excessively by arbitrary selections. Therefore, thoughtfully crafted priors remain indispensable tools for stabilizing convergence behavior, preventing overfitting, promoting identifiability, and facilitating principled interpretations rooted in defensible epistemological grounds.</p></li>
<li><p><strong>Prior Strength</strong>: Depending on the degree of conviction conveyed through the prior, stronger priors tend to dominate posteriors whenever confronted with scanty data containing limited signal strength. Conversely, feeble priors carrying negligible persuasion recede into oblivion rapidly once substantial amounts of informative data emerge. Hence, careful calibration of prior strengths ensures harmonious fusion of prior knowledge and empirical discoveries, culminating in mutually reinforced syntheses reflecting augmented wisdom instead of discordant contradictions.</p></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Impact of Prior Distributions on Statistical Inferences**</p>
</blockquote>
<ol type="1">
<li><p><strong>Prior Sensitivity</strong>: The choice of prior distribution significantly influences posterior estimates, especially in Bayesian analysis. This sensitivity to priors is more pronounced when data is scarce or model complexity is high.</p></li>
<li><p><strong>Data Volume</strong>: With abundant data, the influence of the prior diminishes, allowing the data to “speak for themselves”. In contrast, with limited data, the prior plays a crucial role in shaping the posterior.</p></li>
<li><p><strong>Model Complexity</strong>: In more complex models, the prior can guide the estimation process, helping to avoid overfitting. It acts as a regularising agent, balancing empirical evidence against over-complexity.</p></li>
<li><p><strong>Prior Strength</strong>: Strong, informative priors assert more influence, potentially overshadowing data, while weak, non-informative priors allow data to have greater sway. The choice between strong and weak priors depends on the level of pre-existing knowledge and the goal of the analysis.</p></li>
<li><p><strong>Convergence of Posteriors</strong>: The prior impacts the rate and stability of convergence to the posterior distribution. Appropriate priors can facilitate faster and more stable convergence, particularly important in complex or data-sparse situations.</p></li>
<li><p><strong>Balance of Knowledge and Evidence</strong>: Priors represent a blend of existing knowledge with new evidence. The tension between relying on prior knowledge and empirical data is a fundamental aspect of Bayesian inference, guiding the interpretation and robustness of statistical conclusions.</p></li>
</ol>
<p>In summary, prior distributions are a cornerstone in Bayesian inference, influencing the robustness, interpretability, and convergence of statistical models, especially when faced with complex models or limited data. Their thoughtful selection is vital for making sound inferences.</p>
</div>
</div>
</section>
</section>
<section id="example-bayesian-inference-in-investment-decision-making" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="example-bayesian-inference-in-investment-decision-making"><span class="header-section-number">6.2.2</span> Example: Bayesian Inference in Investment Decision-Making</h3>
<p>Imagine a scenario in the financial world where an investment firm is analysing the performance of a new financial product, which can either be profitable (“success”) or not. The firm has limited performance data: the product has been tested in five different market conditions, yielding four profitable outcomes. To evaluate the product’s future profitability, we employ Bayesian inference, starting with a beta prior to represent our initial uncertainty about the product’s performance.</p>
<p><strong>Binomial Likelihood: Investment Performance</strong></p>
<p>Consider the test of the financial product under five different market conditions, with each condition independently yielding profit with an unknown true probability ( ). Let’s say the product was profitable in four out of these five tests. The likelihood of observing this outcome, assuming a binomial process, is represented as:</p>
<p>[ f(D|) = <sup>4(1-)</sup>1 = 5<sup>4(1-)</sup>1, ] where ( D = {“Profit, Profit, Profit, Profit, Loss”} ) denotes the observed sequence of outcomes.</p>
<p><strong>Beta Prior: Initial Belief about Profitability</strong></p>
<p>Our initial belief about the product’s performance is encoded in a beta prior. This prior is chosen due to its conjugate relationship with the binomial likelihood, facilitating easier calculation of the posterior. With hyperparameters ( (, ) = (3, 3) ), it reflects a balanced view between profitability and non-profitability:</p>
<p>[ () = <sup>2(1-)</sup>2. ]</p>
<p><strong>Posterior Distribution: Updated Belief</strong></p>
<p>Using Bayes’ rule, we update our belief based on the observed data:</p>
<p>[ p(|D) <sup>{6-1}(1-)</sup>{3-1} (6, 3). ]</p>
<p>This posterior distribution is a Beta distribution with parameters ( (6, 3) ), indicating an updated belief about the product’s profitability.</p>
<p><strong>Summary Statistics: Interpreting the Posterior</strong></p>
<p>From the posterior distribution, we derive:</p>
<ul>
<li>Mean: ( () = ).</li>
<li>Mode: ( () = ).</li>
<li>Variance: ( () = ).</li>
</ul>
<p>These statistics suggest a revised understanding that the product is more likely to be profitable than not, with the mean and mode indicating over 60% likelihood of profit in future tests. The relatively low variance points to increased confidence in this assessment compared to the initial uncertainty.</p>
<p><strong>Conclusion: Bayesian Inference in Finance</strong></p>
<p>This example illustrates how Bayesian methods can be applied in finance to update beliefs about a product’s performance based on limited data. The approach helps investment analysts to quantify their uncertainty and adjust their expectations as more data becomes available, thereby aiding in more informed decision-making.</p>
<p>Certainly! Below is an example in R that demonstrates how to perform Bayesian inference for the given finance scenario using a binomial likelihood and a beta prior. The code is well-commented to emphasize its modular approach.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bayesian Inference in Finance: Evaluating a Financial Product's Profitability</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load necessary library</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Define the Prior</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Using a Beta distribution with parameters alpha = 3 and beta = 3</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>alpha_prior <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>beta_prior <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Define the Likelihood</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Based on the observed data: 4 profits out of 5 trials</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>successes <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>trials <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Calculate the Posterior</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># For Beta-Binomial, the posterior parameters are updated simply</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>alpha_posterior <span class="ot">&lt;-</span> alpha_prior <span class="sc">+</span> successes</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>beta_posterior <span class="ot">&lt;-</span> beta_prior <span class="sc">+</span> trials <span class="sc">-</span> successes</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Generate Posterior Distribution</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>theta_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>posterior_distribution <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(theta_values, alpha_posterior, beta_posterior)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Plot the Posterior Distribution</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta_values, posterior_distribution, <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="st">"Theta (Probability of Profit)"</span>, </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Density"</span>, <span class="at">main =</span> <span class="st">"Posterior Distribution"</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> (alpha_posterior<span class="dv">-1</span>)<span class="sc">/</span>(alpha_posterior<span class="sc">+</span>beta_posterior<span class="dv">-2</span>), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Calculate and Print Summary Statistics</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>posterior_mean <span class="ot">&lt;-</span> alpha_posterior <span class="sc">/</span> (alpha_posterior <span class="sc">+</span> beta_posterior)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>posterior_mode <span class="ot">&lt;-</span> (alpha_posterior <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (alpha_posterior <span class="sc">+</span> beta_posterior <span class="sc">-</span> <span class="dv">2</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>posterior_variance <span class="ot">&lt;-</span> (alpha_posterior <span class="sc">*</span> beta_posterior) <span class="sc">/</span> </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>                      ((alpha_posterior <span class="sc">+</span> beta_posterior)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> (alpha_posterior <span class="sc">+</span> beta_posterior <span class="sc">+</span> <span class="dv">1</span>))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Posterior Mean: "</span>, posterior_mean, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Posterior Mode: "</span>, posterior_mode, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Posterior Variance: "</span>, posterior_variance, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Conclusion: The summary statistics provide an updated belief about the product's profitability.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="explanation" class="level4" data-number="6.2.2.1">
<h4 data-number="6.2.2.1" class="anchored" data-anchor-id="explanation"><span class="header-section-number">6.2.2.1</span> Explanation:</h4>
<ol type="1">
<li><strong>Load Library</strong>: We use <code>ggplot2</code> for plotting. If not installed, it gets installed first.</li>
<li><strong>Define the Prior</strong>: A beta distribution with parameters alpha and beta, both set to 3, representing our initial neutral belief about the product’s performance.</li>
<li><strong>Define the Likelihood</strong>: Based on observed data, here 4 out of 5 trials were successful (profitable).</li>
<li><strong>Calculate the Posterior</strong>: Update the alpha and beta parameters of the beta distribution based on the observed data.</li>
<li><strong>Generate Posterior Distribution</strong>: Create a range of theta values (probability of success) and calculate the density of these values under the posterior distribution.</li>
<li><strong>Plot the Posterior Distribution</strong>: Visualize the updated belief about the product’s profitability.</li>
<li><strong>Calculate and Print Summary Statistics</strong>: Derive the mean, mode, and variance from the posterior to summarize our updated belief.</li>
<li><strong>Conclusion</strong>: These statistics offer insights into the likelihood of the product being profitable under future market conditions.</li>
</ol>
<p>This R code provides a clear, modular approach to Bayesian inference, guiding through each step from prior selection to posterior analysis, particularly useful in financial decision-making contexts.</p>
</section>
</section>
</section>
<section id="bayesian-inference-for-univariate-normal-models-expanded" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="bayesian-inference-for-univariate-normal-models-expanded"><span class="header-section-number">6.3</span> Bayesian Inference for Univariate Normal Models (Expanded)</h2>
<p>In this section we delve deeper into Bayesian inference for univariate normal models, covering analytical derivations, graphical representations, moment calculations, and sampling techniques for approximating the posterior. We also touch upon Bayesian credible intervals, with a special focus on Highest Posterior Density (HPD) intervals. Throughout this section, we sprinkle in some <code>R</code> examples relevant to the finance context.</p>
<section id="deriving-the-posterior-density-analytically" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="deriving-the-posterior-density-analytically"><span class="header-section-number">6.3.1</span> Deriving the Posterior Density Analytically</h3>
<p>Assume we want to estimate the expected return of a company, denoted by <span class="math inline">\(\mu\)</span>. We collect monthly returns, <span class="math inline">\(X=(x\_1,...,x\_n)\)</span>, assumed to follow a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and known precision <span class="math inline">\(\tau\)</span>. We adopt a normal prior for <span class="math inline">\(\mu\)</span>, denoted as <span class="math inline">\(\mu\_0 \sim \mathcal{N}(\mu\_p, \tau\_p^{-1})\)</span>, where <span class="math inline">\(\mu\_p\)</span> is the prior mean and <span class="math inline">\(\tau\_p\)</span> is the prior precision. Invoking Bayes’ Rule, we derive the posterior distribution:</p>
<p><span class="math display">\[
\mu \mid X \sim \mathcal{N}\left( \frac{\tau\_p \mu\_p + n\bar{x}\,\tau}{\tau\_p + n\tau},\; (\tau\_p + n\tau)^{-1} \right)
\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> stands for the sample mean.</p>
<p>Using <code>R</code>, we can implement the posterior distribution for a toy example with fictional returns and prior settings:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Toy data: Monthly returns</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>monthly_returns <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.02</span>, <span class="fl">0.01</span>, <span class="sc">-</span><span class="fl">0.03</span>, <span class="fl">0.04</span>, <span class="fl">0.01</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>sample_size <span class="ot">&lt;-</span> <span class="fu">length</span>(monthly_returns)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior settings</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>prior_mean <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>prior_precision <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Precision</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>precision <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="fu">sd</span>(monthly_returns)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Updated mean and precision for posterior</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>updated_mean <span class="ot">&lt;-</span> (prior_precision <span class="sc">*</span> prior_mean <span class="sc">+</span> sample_size <span class="sc">*</span> <span class="fu">mean</span>(monthly_returns) <span class="sc">*</span> precision) <span class="sc">/</span> (prior_precision <span class="sc">+</span> sample_size <span class="sc">*</span> precision)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>updated_precision <span class="ot">&lt;-</span> prior_precision <span class="sc">+</span> precision <span class="sc">*</span> sample_size</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the result</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Updated mean:"</span>, <span class="fu">round</span>(updated_mean, <span class="dv">5</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Updated precision:"</span>, <span class="fu">round</span>(updated_precision, <span class="dv">5</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="expressions-for-the-normal-and-inverse-gamma-distributions" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="expressions-for-the-normal-and-inverse-gamma-distributions"><span class="header-section-number">6.3.2</span> Expressions for the Normal and Inverse-Gamma Distributions</h3>
<p>Two important distributions play a crucial role in Bayesian inference:</p>
<ul>
<li><p><strong>Normal distribution</strong>, denoted as <span class="math inline">\(\mathcal{N}(\mu, \tau^{-1})\)</span>: <span class="math display">\[
f(x;\mu, \tau) = \sqrt{\frac{\tau}{2\pi}} \exp \left\{ -\frac{\tau}{2} (x - \mu)^2 \right\}
\]</span></p></li>
<li><p><strong>Inverse-gamma distribution</strong>, denoted as <span class="math inline">\(\mathcal{IG}(\alpha, \beta)\)</span>: <span class="math display">\[
f(x;\alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha-1} \exp \left\{ -\frac{\beta}{x} \right\}
\]</span></p></li>
</ul>
<p>These distributions enable us to handle numerous Bayesian problems elegantly.</p>
</section>
<section id="graphical-representation-of-densities" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="graphical-representation-of-densities"><span class="header-section-number">6.3.3</span> Graphical Representation of Densities</h3>
<p>Visuals aid our understanding of probability distributions. We can easily generate graphical representations of normal and inverse-gamma distributions using <code>R</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for normal density</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>pdf_normal <span class="ot">&lt;-</span> <span class="cf">function</span>(x, mu, tau) {</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sqrt</span>(tau<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>pi)) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>tau<span class="sc">*</span>(x<span class="sc">-</span>mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for inverse-gamma density</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>pdf_invgamma <span class="ot">&lt;-</span> <span class="cf">function</span>(x, alpha, beta) {</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  (beta<span class="sc">^</span>alpha)<span class="sc">/</span><span class="fu">gamma</span>(alpha) <span class="sc">*</span> x<span class="sc">^-</span>(alpha<span class="sc">+</span><span class="dv">1</span>) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>beta<span class="sc">/</span>x)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Range for x axis</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>xrange <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot normal density</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xrange, <span class="fu">sapply</span>(xrange, pdf_normal, <span class="at">mu =</span> <span class="dv">0</span>, <span class="at">tau =</span> <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">ylab =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">"x"</span>, <span class="at">main=</span><span class="st">"Normal Distribution"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot inverse-gamma density</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xrange, <span class="fu">sapply</span>(xrange, pdf_invgamma, <span class="at">alpha =</span> <span class="dv">2</span>, <span class="at">beta =</span> <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">ylab =</span> <span class="st">""</span>, <span class="at">xlab =</span> <span class="st">"x"</span>, <span class="at">main=</span><span class="st">"Inverse-Gamma Distribution"</span>, <span class="at">yaxt=</span><span class="st">'n'</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side=</span><span class="dv">2</span>, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="calculating-moments" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="calculating-moments"><span class="header-section-number">6.3.4</span> Calculating Moments</h3>
<p>Computing moments like mean, variance, skewness, and kurtosis provides valuable insights into the distribution’s properties. Though analytical expressions exist for many distributions, numerical methods serve as alternatives for complex distributions.</p>
</section>
<section id="sampling-from-the-joint-posterior-distribution" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="sampling-from-the-joint-posterior-distribution"><span class="header-section-number">6.3.5</span> Sampling from the Joint Posterior Distribution</h3>
<p>Three widely-used methods allow us to approximate the posterior distribution:</p>
<ol type="1">
<li><strong><em>Grid approximation</em></strong> partitions the parameter space into a fine grid, determining the posterior density at each grid point. Despite ease of understanding and implementation, grid approximation faces issues with low accuracy and poor scalability.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Finite grids for mu and tau</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>mus <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>taus <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.5</span>, <span class="fl">1.5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute posterior density</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>joint_posterior_values <span class="ot">&lt;-</span> <span class="fu">outer</span>(mus, taus, <span class="cf">function</span>(mu, tau) <span class="fu">dnorm</span>(mu, <span class="at">mean =</span> updated_mean, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>tau)))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape joint_posterior_values into a matrix</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>joint_posterior_matrix <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(joint_posterior_values)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Find index of maximum value</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>max_index <span class="ot">&lt;-</span> <span class="fu">which.max</span>(joint_posterior_matrix)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve estimated mu and tau</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>estimated_mu <span class="ot">&lt;-</span> mus[<span class="fu">floor</span>(max_index <span class="sc">%/%</span> <span class="fu">ncol</span>(joint_posterior_matrix)) <span class="sc">+</span> <span class="dv">1</span>]</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>estimated_tau <span class="ot">&lt;-</span> taus[max_index <span class="sc">%%</span> <span class="fu">ncol</span>(joint_posterior_matrix) <span class="sc">+</span> <span class="dv">1</span>]</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Estimated mu:"</span>, <span class="fu">round</span>(estimated_mu, <span class="dv">5</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Estimated tau:"</span>, <span class="fu">round</span>(estimated_tau, <span class="dv">5</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li><p><strong><em>Monte Carlo integration</em></strong> randomly draws samples from the posterior distribution, avoiding explicit evaluation of the density. Effective for high-dimensional problems, Monte Carlo integration demands copious samples to deliver decent accuracy.</p></li>
<li><p><strong><em>Importance sampling</em></strong> proposes a distribution targeting regions with considerable posterior mass. Drawing samples from this distribution, importance sampling wisely allocates computational resources, particularly helpful for challenging posteriors.</p></li>
</ol>
</section>
<section id="bayesian-credible-intervals" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="bayesian-credible-intervals"><span class="header-section-number">6.3.6</span> Bayesian Credible Intervals</h3>
<p>Similar to classical confidence intervals, Bayesian credible intervals bound the uncertain parameter within a plausible range. Among them, Highest Posterior Density (HPD) intervals stand out.</p>
<section id="highest-posterior-density-hpd-interval-calculation" class="level4" data-number="6.3.6.1">
<h4 data-number="6.3.6.1" class="anchored" data-anchor-id="highest-posterior-density-hpd-interval-calculation"><span class="header-section-number">6.3.6.1</span> Highest Posterior Density (HPD) Interval Calculation</h4>
<p>An HPD interval surrounds the most probable parameter values with the least width necessary for a given credibility level. No other interval holds a higher concentration of probability mass.</p>
<p>Consider a univariate normal model with an unknown mean, <span class="math inline">\(\mu\)</span>, and known precision, <span class="math inline">\(\tau\)</span>. Given the posterior distribution, <span class="math inline">\(\mu \mid X \sim \mathcal{N}(\hat{\mu}, \hat{\sigma}^2)\)</span>, finding the HPD interval involves solving the following inequality:</p>
<p><span class="math display">\[
\Phi \left( \frac{\hat{\mu}_u - \hat{\mu}}{\hat{\sigma}} \right) - \Phi \left( \frac{\hat{\mu}_l - \hat{\mu}}{\hat{\sigma}} \right) = \gamma
\]</span></p>
<p>where <span class="math inline">\(\Phi(\cdot)\)</span> denotes the standard normal cumulative distribution function, <span class="math inline">\(\hat{\mu}_u\)</span> and <span class="math inline">\(\hat{\mu}_l\)</span> denote the upper and lower limits of the interval, and <span class="math inline">\(\gamma\)</span> is the credibility level.</p>
</section>
<section id="hpd-properties-compared-to-classical-confidence-intervals" class="level4" data-number="6.3.6.2">
<h4 data-number="6.3.6.2" class="anchored" data-anchor-id="hpd-properties-compared-to-classical-confidence-intervals"><span class="header-section-number">6.3.6.2</span> HPD Properties Compared to Classical Confidence Intervals</h4>
<p>Key differences separate HPD intervals from classical confidence intervals. Mainly, HPD intervals utilize the whole posterior distribution, granting direct probabilistic interpretation. Meanwhile, classical confidence intervals solely deal with sampling-induced variation, neglecting prior information.</p>
<p>Stay tuned for tomorrow’s continuation, where we explore hierarchical modeling in depth. Until then!</p>
</section>
</section>
</section>
<section id="motivation-for-hierarchical-models" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="motivation-for-hierarchical-models"><span class="header-section-number">6.4</span> Motivation for Hierarchical Models</h2>
<p>Hierarchical models, sometimes referred to as multilevel models, recognize that data is often organized in distinct groups or clusters, and observations within those groups tend to be more alike compared to observations outside of the groups. Ignoring the hierarchical structure can lead to incorrect inferences, loss of efficiency, and inflated Type I error rates.</p>
<p><strong>Pooling Information Across Groups</strong></p>
<p>One major advantage of hierarchical models is the ability to pool information across groups, borrowing strength from neighboring groups. This technique prevents overfitting, improves parameter estimates, and reduces the chances of getting implausibly large or small coefficient estimates.</p>
<p><strong>Specifying a Hierarchical Structure</strong></p>
<p>Before fitting a hierarchical model, you must identify the grouping structure and decide how to model the dependence between observations within groups. This step involves specifying a hierarchical structure consisting of different levels connected by linkages.</p>
<p><strong>Defining Submodels Within Levels of Hierarchy</strong></p>
<p>Each level within the hierarchical structure consists of submodels with their own parameters. Lower-level submodels may contain covariates measured at the lowest level, while higher-level submodels include group-level predictors. Covariates at different levels can interact, and cross-classified designs are allowed.</p>
<p><strong>Linkages Between Layers</strong></p>
<p>Linkages bind together the different levels of the hierarchy. There are three common linkages:</p>
<ol type="1">
<li><em>Fixed</em>: Parameters at higher levels are treated as constants, unaffected by the lower levels.</li>
<li><em>Random</em>: Parameters at higher levels are considered random variables, varying between groups according to a specific probability distribution.</li>
<li><em>Cross-Level:</em> Includes interactions and predictors between different levels of the hierarchy, allowing for more complex relationships.</li>
</ol>
<p><strong>Common Structures</strong></p>
<p>There are various hierarchical structures, ranging from simple two-level hierarchies to more complex three-level and beyond.</p>
<p><strong>Two-Level Hierarchies</strong></p>
<p>In a two-level hierarchy, there are two levels of organization: individuals and groups (clusters):</p>
<p><span class="math display">\[
y_{ij} = \beta_{0j} + \beta_{1j}x_{ij} + \varepsilon_{ij}
\]</span></p>
<p>where <span class="math inline">\(\beta_{0j}\)</span> and <span class="math inline">\(\beta_{1j}\)</span> are unique to each group j. One option to model the group-specific slope and intercept is to treat them as random effects:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_{0j} &amp;= \gamma_{00} + U_{0j} \\
\beta_{1j} &amp;= \gamma_{10} + U_{1j}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\gamma_{00}\)</span> and <span class="math inline">\(\gamma_{10}\)</span> are the overall intercept and slope, while <span class="math inline">\(U_{0j}\)</span> and <span class="math inline">\(U_{1j}\)</span> are random effects shared by members of the same cluster j.</p>
<p><strong>Three-Level Hierarchies</strong></p>
<p>Three-level hierarchies extend the idea of nesting to a third layer, adding another level of complexity. For example, you might have students (first level) nested within classrooms (second level), which are themselves nested within schools (third level).</p>
<section id="examples-in-r" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="examples-in-r"><span class="header-section-number">6.4.1</span> Examples in R</h3>
<p>Implementing hierarchical models in R can be done using various packages, such as <code>nlme</code>, <code>lme4</code>, and <code>rstanarm</code>. Below is an example of a two-level hierarchical model using <code>lme4</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lme4)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fake data</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">group =</span> letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>], <span class="at">id =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">nrow</span>(dat), <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">each =</span> <span class="dv">10</span>))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">nrow</span>(dat))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit hierarchical model</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lmer</span>(y <span class="sc">~</span> x <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> group), <span class="at">data =</span> dat)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this example, the response variable <code>y</code> depends on the predictor <code>x</code>, and the intercept varies randomly at the <code>group</code> level. The <code>(1 | group)</code> syntax specifies that the intercept is modeled as a random effect.</p>
</section>
<section id="traditional-econometrics-versus-bayesian-hierarchical-models" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="traditional-econometrics-versus-bayesian-hierarchical-models"><span class="header-section-number">6.4.2</span> Traditional econometrics versus Bayesian hierarchical models</h3>
<p>Absolutely, I’ll delve into hierarchical modeling and connect it to the frequentist approach using fixed and random effect estimators. We will go through the motivation, followed by an example implemented in <code>R</code> to demonstrate the efficiency and consistency of hierarchical models.</p>
<section id="context-and-motivation" class="level6" data-number="6.4.2.0.0.1">
<h6 data-number="6.4.2.0.0.1" class="anchored" data-anchor-id="context-and-motivation"><span class="header-section-number">6.4.2.0.0.1</span> Context and Motivation</h6>
<p>Traditionally, the distinction between fixed and random effects revolves around the assumption of homogeneity versus heterogeneity in the population. In a frequentist framework, fixed effects imply that every level within the population shares the same effect, whereas random effects involve variations across the levels.</p>
<p>However, in practice, this dichotomy isn’t always ideal due to overlapping situations and inconsistent interpretations. Enter hierarchical models, also known as multilevel models, which provide a more holistic perspective by explicitly considering the dependency among units. Instead of forcing a hard separation, hierarchical models blend the ideas of fixed and random effects smoothly, offering improved flexibility and consistency.</p>
</section>
<section id="example-academic-performance-across-schools" class="level6" data-number="6.4.2.0.0.2">
<h6 data-number="6.4.2.0.0.2" class="anchored" data-anchor-id="example-academic-performance-across-schools"><span class="header-section-number">6.4.2.0.0.2</span> Example: Academic Performance Across Schools</h6>
<p>Imagine measuring academic achievement in mathematics assessments across multiple schools. Both frequentist and Bayesian approaches agree that individual students’ scores depend on their innate abilities (fixed effect) and measurement errors. However, the disagreement comes when attributing the variation in math scores across schools. Is it simply random noise, or do schools genuinely vary in their effectiveness, perhaps influenced by resource allocation, teacher quality, curricula, or policies?</p>
<p>Hierarchical models answer this question naturally, capturing the dual nature of both fixed and random effects simultaneously. In the education example, we can describe the achievement of student <span class="math inline">\(i\)</span> in school <span class="math inline">\(j\)</span> as:</p>
<p><span class="math display">\[
y_{ij} = \beta_0 + u_j + \epsilon_{ij}
\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the global mean, <span class="math inline">\(u_j\)</span> accounts for the school-specific offset (random effect), and <span class="math inline">\(\epsilon_{ij}\)</span> covers student-specific measurement error (assumed to be normally distributed).</p>
<p>Our goals consist of estimating the global mean and quantifying the amount of variation explained by schools, captured by the variance of <span class="math inline">\(u_j\)</span>. This way, we maintain the benefits of both fixed and random effects, achieving a more complete and consistent picture.</p>
</section>
</section>
<section id="r-example-frequentist-versus-bayesian-approach" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="r-example-frequentist-versus-bayesian-approach"><span class="header-section-number">6.4.3</span> R Example: Frequentist Versus Bayesian Approach</h3>
<p>We’ll first look at a frequentist approach using the <code>lme4</code> package, followed by a Bayesian version using <code>rstanarm</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load libraries</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lme4)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstanarm)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating fake data</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>n_students <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>n_schools <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>global_mean <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>school_effects <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_schools, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>student_errors <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_students <span class="sc">*</span> n_schools, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">10</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>math_scores <span class="ot">&lt;-</span> global_mean <span class="sc">+</span> school_effects[student_scores <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_schools, <span class="at">replace =</span> <span class="cn">TRUE</span>)] <span class="sc">+</span> student_errors</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Stack data</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>stacked_data <span class="ot">&lt;-</span> <span class="fu">stack</span>(<span class="fu">data.frame</span>(<span class="at">Math_Score =</span> math_scores))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>stacked_data<span class="sc">$</span>Student_ID <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_students, <span class="at">each =</span> n_schools)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>stacked_data<span class="sc">$</span>School_ID <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_schools, <span class="at">times =</span> n_students)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Frequentist approach</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>lm_model <span class="ot">&lt;-</span> <span class="fu">lmer</span>(values <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> School_ID), <span class="at">data =</span> stacked_data)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_model)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Bayesian approach</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>bym_model <span class="ot">&lt;-</span> <span class="fu">stan_glmer</span>(values <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> School_ID), <span class="at">data =</span> stacked_data, <span class="at">family =</span> <span class="fu">gaussian</span>())</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(bym_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Both approaches reveal comparable estimates for the global mean and variance components. Yet, notice that the hierarchical model handles both fixed and random effects concurrently, improving interpretability and consistency.</p>
<p>So far, we’ve covered a lot of ground, gradually unfolding the mysteries of hierarchical models, their connection to fixed and random effects, and their advantages in the context of frequentist and Bayesian approaches. Don’t forget to tune in tomorrow as we embark on another thrilling adventure in the realm of Bayesian modeling!</p>
</section>
</section>
<section id="mcmc-methods" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="mcmc-methods"><span class="header-section-number">6.5</span> MCMC Methods</h2>
<p>Markov Chain Monte Carlo (MCMC) methods are a collection of algorithms for sampling from complex probability distributions. These methods are extensively used in Bayesian inference to generate draws from the posterior distribution, especially in high-dimensional settings where analytical solutions aren’t feasible.</p>
<p><strong>What is MCMC?</strong></p>
<p>At its core, MCMC leverages the Markov property, stating that the probability of transitioning to the next state depends only on the current state and not on the history of previous states. Starting from an initial guess, MCMC builds a chain of samples that ultimately converge to the target distribution.</p>
<p><strong>Simulating draws from complex distributions</strong></p>
<p>MCMC excels at generating samples from complex distributions that lack closed-form solutions. This capability makes MCMC an attractive option for Bayesian statisticians dealing with intricate models and hierarchical structures.</p>
<p><strong>Types of MCMC methods</strong></p>
<p>There are various flavors of MCMC methods, each catering to different scenarios and requirements.</p>
<ol type="1">
<li>Metropolis-Hastings</li>
</ol>
<p>The Metropolis-Hastings algorithm is a generic MCMC method that accepts or rejects proposals based on an acceptance ratio. The proposal distribution determines the candidates for the next state, while the acceptance ratio governs whether to accept or reject the proposal.</p>
<ol start="2" type="1">
<li>Gibbs sampling</li>
</ol>
<p>Gibbs sampling focuses on sampling blocks of parameters instead of individual parameters. This strategy breaks down the problem into simpler chunks and can improve the mixing of the chain.</p>
<ol start="3" type="1">
<li>Hamiltonian Monte Carlo (HMC)</li>
</ol>
<p>Hamiltonian Monte Carlo (HMC) combines gradient information with random walks to propose new states. By exploiting the geometry of the target distribution, HMC takes longer strides in promising directions, reducing the correlation between consecutive samples and speeding up convergence.</p>
<p><strong>Advantages and disadvantages</strong></p>
<p>Like any method, MCMC has its pros and cons.</p>
<p><strong>Efficient mixing</strong></p>
<p>Modern MCMC methods, like HMC, efficiently mix across the target distribution, minimizing correlation between consecutive samples and accelerating convergence.</p>
<p><strong>Correlated samples</strong></p>
<p>Despite their strengths, MCMC methods produce correlated samples, meaning that adjacent samples carry redundant information. This drawback necessitates thinning the chain, removing intermediate samples to reduce serial correlation.</p>
<p><strong>Tuning parameters</strong></p>
<p>Some MCMC methods require manual tuning of parameters, like proposal distribution scales or leap sizes. Improper tuning can negatively affect the mixing and convergence of the chain, demanding user intervention and judgment calls.</p>
<section id="example-in-r-simple-random-walk-metropolis-hastings-sampler" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="example-in-r-simple-random-walk-metropolis-hastings-sampler"><span class="header-section-number">6.5.1</span> Example in R: Simple random walk Metropolis-Hastings sampler</h3>
<p>Here’s an example of a simple Metropolis-Hastings algorithm in R, implementing a random walk proposal for a univariate distribution:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple Metropolis-Hastings sampler with random walk proposal</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>metrop_rw <span class="ot">&lt;-</span> <span class="cf">function</span>(target_density, initial_state, niter, <span class="at">proposal_scale =</span> <span class="dv">1</span>) {</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  states <span class="ot">&lt;-</span> <span class="fu">numeric</span>(niter)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  curr_state <span class="ot">&lt;-</span> initial_state</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>niter) {</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    prop_state <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> curr_state, <span class="at">sd =</span> proposal_scale)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    acceptance_ratio <span class="ot">&lt;-</span> <span class="fu">target_density</span>(prop_state) <span class="sc">/</span> <span class="fu">target_density</span>(curr_state)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    rand_num <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">log</span>(rand_num) <span class="sc">&lt;</span> <span class="fu">log</span>(acceptance_ratio)) {</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>      curr_state <span class="ot">&lt;-</span> prop_state</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    states[i] <span class="ot">&lt;-</span> curr_state</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(states)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Test function</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>target_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Replace with your target distribution</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>initial_state <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">metrop_rw</span>(target_fun, initial_state, <span class="dv">1000</span>, <span class="fl">0.5</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Plot trace plot</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(samples, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"Trace plot for Metropolis-Hastings"</span>, <span class="at">xlab =</span> <span class="st">"Iteration"</span>, <span class="at">ylab =</span> <span class="st">"State"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Replace <code>target_fun</code> with your desired target distribution. This example implements a simple Metropolis-Hastings sampler using a random walk proposal. Users can adjust the proposal scale and initial state. Remember to properly tune the proposal scale for effective mixing and convergence.</p>
</section>
</section>
<section id="bayesian-approaches-to-model-financial-data" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="bayesian-approaches-to-model-financial-data"><span class="header-section-number">6.6</span> Bayesian Approaches to Model Financial Data</h2>
<p>In this lecture, we delve into various Bayesian time series models and volatility models that are widely used in finance for modeling financial data. We’ll discuss the basic concepts of these models and how they differ from their frequentist counterparts.</p>
<p><strong>Bayesian Time Series Models</strong></p>
<ul>
<li><strong>Autoregressive (AR)</strong> models are a class of time series models where the dependent variable is a linear combination of lagged observations and white noise. An AR(1) model, for instance, has the form:</li>
</ul>
<p><span class="math display">\[y_t = \phi y_{t-1} + \epsilon_t\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is the autoregressive parameter and <span class="math inline">\(\epsilon_t\)</span> is the white noise term.</p>
<ul>
<li><strong>Moving Average (MA)</strong> models are another class of time series models where the dependent variable is a linear combination of current and lagged white noise terms. An MA(1) model, for example, can be written as:</li>
</ul>
<p><span class="math display">\[y_t = \theta \epsilon_{t-1} + \epsilon_t\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the moving average parameter and <span class="math inline">\(\epsilon_t\)</span> is the white noise term.</p>
<ul>
<li><strong>Autoregressive Moving Average (ARMA)</strong> models combine elements of both AR and MA models. An ARMA(1,1) model, for instance, has the form:</li>
</ul>
<p><span class="math display">\[y_t = \phi y_{t-1} + \theta \epsilon_{t-1} + \epsilon_t\]</span></p>
<ul>
<li><strong>Autoregressive Integrated Moving Average (ARIMA)</strong> models extend ARIMA models by introducing differencing to remove trend and seasonality from the time series. An ARIMA(1,1,1) model, for example, has the form:</li>
</ul>
<p><span class="math display">\[\nabla y_t = \phi \nabla y_{t-1} + \theta \epsilon_{t-1} + \epsilon_t\]</span></p>
<p>where <span class="math inline">\(\nabla\)</span> is the differencing operator.</p>
<p><strong>Volatility Models</strong></p>
<ul>
<li><strong>Generalized Autoregressive Conditional Heteroskedasticity (GARCH)</strong> models are widely used in finance to model volatility. GARCH models assume that the variance of the error term changes over time, depending on past error terms. The basic GARCH(1,1) model can be written as:</li>
</ul>
<p><span class="math display">\[\sigma_t^2 = \omega + \alpha \epsilon_{t-1}^2 + \beta \sigma_{t-1}^2\]</span></p>
<p>where <span class="math inline">\(\sigma_t^2\)</span> is the variance at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\omega\)</span> is the constant, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are parameters, and <span class="math inline">\(\epsilon_{t-1}^2\)</span> and <span class="math inline">\(\sigma_{t-1}^2\)</span> are the squared error term and variance at time <span class="math inline">\(t-1\)</span>, respectively.</p>
<ul>
<li><strong>Exponentiated GARCH (EGARCH)</strong> models are a variant of GARCH models that allow for asymmetric responses to shocks. The basic EGARCH(1,1) model can be written as:</li>
</ul>
<p><span class="math display">\[\log(\sigma_t^2) = \omega + \alpha |\frac{\epsilon_{t-1}}{\sigma_{t-1}}| + \gamma \frac{\epsilon_{t-1}}{\sigma_{t-1}} + \beta \log(\sigma_{t-1}^2)\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> controls the leverage effect.</p>
<ul>
<li><strong>Stochastic Volatility (SV)</strong> models are another class of models used to model volatility in finance. SV models assume that the variance is a latent stochastic process that affects the observed data. The basic SV model can be written as:</li>
</ul>
<p><span class="math display">\[y_t = \exp\{\frac{h_t}{2}\} z_t\]</span></p>
<p><span class="math display">\[h_t = \mu + \phi (h_{t-1}-\mu) + \eta_t\]</span></p>
<p>where <span class="math inline">\(h_t\)</span> is the log-variance, <span class="math inline">\(z_t\)</span> is a standard normal variable, and <span class="math inline">\(\eta_t\)</span> is another disturbance term.</p>
<section id="ar1-example-in-r" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="ar1-example-in-r"><span class="header-section-number">6.6.1</span> AR(1) Example in R</h3>
<p>Sure, I will provide detailed explanations for the code used to simulate the AR(1) data and fit the Bayesian AR(1) model using both the <code>brms</code> and <code>rstanarm</code> packages.</p>
<ul>
<li>Step 1: Simulate AR(1) Process</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># Set seed for reproducibility</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># Sample size</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>phi <span class="ot">&lt;-</span> <span class="fl">0.75</span> <span class="co"># Phi parameter</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co"># Standard deviation of error term</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a time series</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>time_series <span class="ot">&lt;-</span> <span class="fu">arima.sim</span>(<span class="at">n =</span> n, <span class="fu">list</span>(<span class="at">order =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">ma =</span> <span class="fu">c</span>(phi)), <span class="at">innov =</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>time_series <span class="ot">&lt;-</span> <span class="fu">ts</span>(time_series, <span class="at">start =</span> <span class="dv">1</span>, <span class="at">frequency =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li><code>set.seed(123)</code>: Sets the seed for the random number generator to ensure reproducibility.</li>
<li><code>n &lt;- 100</code>: Declares the sample size for the simulated time series.</li>
<li><code>phi &lt;- 0.75</code>: Specifies the phi parameter of the AR(1) process.</li>
<li><code>sigma &lt;- 1</code>: Determines the standard deviation of the innovation term (error term) added to the AR(1) process.</li>
<li><code>arima.sim()</code>: Creates a simulated ARIMA process. Here, we specify an AR(1) process by passing an ordered pair of <code>c(1, 0, 0)</code> for the <code>order</code> parameter and a scalar <code>c(phi)</code> for the <code>ma</code> parameter. The <code>innov</code> parameter defines the error term, which is created using <code>rnorm()</code> with mean 0 and standard deviation <code>sigma</code>.</li>
</ol>
<ul>
<li>Step 2: Fit the Bayesian AR(1) Model using <code>brms</code></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(brms)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>fit_brms <span class="ot">&lt;-</span> <span class="fu">brm</span>(time_series <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">ar</span>(<span class="dv">1</span>), <span class="at">data =</span> <span class="fu">data.frame</span>(time_series), <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">200</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.95</span>), <span class="at">backend =</span> <span class="st">"cmdstanr"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_brms)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_brms)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">pp_check</span>(fit_brms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li><code>library(brms)</code>: Loads the <code>brms</code> package for Bayesian modeling in R.</li>
<li><code>fit_brms &lt;- brm(...)</code>: Uses the <code>brm()</code> function to fit the Bayesian AR(1) model. The formula passed to the function is <code>time_series ~ 1 + ar(1)</code>, indicating that the response variable is <code>time_series</code>, and we have a simple intercept and an AR(1) term included in the model.</li>
<li><code>data = data.frame(time_series)</code>: Passes the time series data as a data frame to the <code>brm()</code> function.</li>
<li><code>chains = 4, cores = 4, iter = 2000, control = list(adapt_delta = 0.95), backend = "cmdstanr"</code>: Configures various settings for running the MCMC algorithm:
<ul>
<li><code>chains</code>: The number of parallel MCMC chains to execute.</li>
<li><code>cores</code>: The number of CPU cores to dedicate to each MCMC chain.</li>
<li><code>iter</code>: The total number of iterations per MCMC chain.</li>
<li><code>control</code>: Allows fine-grained configuration of MCMC settings, here adapted delta is set to 0.95.</li>
<li><code>backend</code>: The backend engine to use for MCMC, cmdstanr is used here.</li>
</ul></li>
<li><code>summary(fit_brms)</code>: Summarizes the posterior distribution of the parameters in the model.</li>
<li><code>plot(fit_brms)</code>: Plots the posterior distributions of the parameters.</li>
<li><code>pp_check(fit_brms)</code>: Performs posterior predictive checks to validate the goodness of fit of the model.</li>
</ol>
<ul>
<li>Step 3: Fit the Bayesian AR(1) Model using <code>rstanarm</code></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages("rstanarm")</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstanarm)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>fit_rstanarm <span class="ot">&lt;-</span> <span class="fu">stan_glmer</span>(time_series <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> time_series), <span class="at">data =</span> <span class="fu">data.frame</span>(time_series), <span class="at">family =</span> <span class="fu">gaussian</span>(), <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">200</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.95</span>))</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_rstanarm)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_rstanarm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li><code>install.packages("rstanarm")</code>: Installs the <code>rstanarm</code> package for Bayesian modeling in R, if it hasn’t already been installed.</li>
<li><code>library(rstanarm)</code>: Loads the <code>rstanarm</code> package for Bayesian modeling in R.</li>
<li><code>fit_rstanarm &lt;- stan_glmer(...)</code>: Uses the <code>stan_glmer()</code> function to fit the Bayesian AR(1) model. The formula passed to the function is <code>time_series ~ 1 + (1 | time_series)</code>, indicating that the response variable is <code>time_series</code>, and we have a simple intercept and a random intercept term <code>(1 | time_series)</code> included in the model.</li>
<li><code>family = gaussian()</code>: Indicates that the response variable follows a normal distribution.</li>
<li><code>chains = 4, cores = 4, iter = 2000, control = list(adapt_delta = 0.95)</code>: Configures various settings for running the MCMC algorithm, similar to the <code>brms</code> example.</li>
<li><code>summary(fit_rstanarm)</code>: Summarizes the posterior distribution of the parameters in the model.</li>
<li><code>plot(fit_rstanarm)</code>: Plots the posterior distributions of the parameters.</li>
</ol>
</section>
<section id="arma11-example-in-r" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="arma11-example-in-r"><span class="header-section-number">6.6.2</span> ARMA(1,1) Example in R</h3>
<ul>
<li>Step 1: Simulate ARMA(1, 1) Process</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>ar1 <span class="ot">&lt;-</span> <span class="fl">0.75</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>ma1 <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a time series</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>arma_series <span class="ot">&lt;-</span> <span class="fu">arima.sim</span>(<span class="at">n =</span> n, <span class="fu">list</span>(<span class="at">ar =</span> ar1, <span class="at">ma =</span> ma1), <span class="at">innov =</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma))</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>arma_series <span class="ot">&lt;-</span> <span class="fu">ts</span>(arma_series, <span class="at">start =</span> <span class="dv">1</span>, <span class="at">frequency =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li><code>set.seed(123)</code>: Sets the seed for the random number generator to ensure reproducibility.</li>
<li><code>n &lt;- 100</code>: Declares the sample size for the simulated time series.</li>
<li><code>ar1 &lt;- 0.75</code>: Specifies the AR(1) parameter of the ARMA(1, 1) process.</li>
<li><code>ma1 &lt;- 0.5</code>: Specifies the MA(1) parameter of the ARMA(1, 1) process.</li>
<li><code>sigma &lt;- 1</code>: Determines the standard deviation of the innovation term (error term) added to the ARMA(1, 1) process.</li>
<li><code>arima.sim()</code>: Creates a simulated ARIMA process. Here, we specify an ARMA(1, 1) process by passing an AR order <code>ar = ar1</code> and MA order <code>ma = ma1</code>. The <code>innov</code> parameter defines the error term, which is created using <code>rnorm()</code> with mean 0 and standard deviation <code>sigma</code>.</li>
</ol>
<ul>
<li>Step 2: Fit the Bayesian ARMA(1, 1) Model using <code>brms</code></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(brms)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>fit_brms <span class="ot">&lt;-</span> <span class="fu">brm</span>(arma_series <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">ar</span>(<span class="dv">1</span>) <span class="sc">+</span> <span class="fu">ma</span>(<span class="dv">1</span>), <span class="at">data =</span> <span class="fu">data.frame</span>(arma_series), <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">200</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.95</span>), <span class="at">backend =</span> <span class="st">"cmdstanr"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_brms)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_brms)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="fu">pp_check</span>(fit_brms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li><code>library(brms)</code>: Loads the <code>brms</code> package for Bayesian modeling in R.</li>
<li><code>fit_brms &lt;- brm(...)</code>: Uses the <code>brm()</code> function to fit the Bayesian ARMA(1, 1) model. The formula passed to the function is <code>arma_series ~ 1 + ar(1) + ma(1)</code>, indicating that the response variable is <code>arma_series</code>, and we have a simple intercept, AR(1) term, and MA(1) term included in the model.</li>
<li><code>data = data.frame(arma_series)</code>: Passes the ARMA(1, 1) data as a data frame to the <code>brm()</code> function.</li>
<li><code>chains = 4, cores = 4, iter = 2000, control = list(adapt_delta = 0.95), backend = "cmdstanr"</code>: Configures various settings for running the MCMC algorithm:
<ul>
<li><code>chains</code>: The number of parallel MCMC chains to execute.</li>
<li><code>cores</code>: The number of CPU cores to dedicate to each MCMC chain.</li>
<li><code>iter</code>: The total number of iterations per MCMC chain.</li>
<li><code>control</code>: Allows fine-grained configuration of MCMC settings, here adapted delta is set to 0.95.</li>
<li><code>backend</code>: The backend engine to use for MCMC, cmdstanr is used here.</li>
</ul></li>
<li><code>summary(fit_brms)</code>: Summarizes the posterior distribution of the parameters in the model.</li>
<li><code>plot(fit_brms)</code>: Plots the posterior distributions of the parameters.</li>
<li><code>pp_check(fit_brms)</code>: Performs posterior predictive checks to validate the goodness of fit of the model.</li>
</ol>
<ul>
<li>Step 3: Fit the Bayesian ARMA(1, 1) Model using <code>rstanarm</code></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages("rstanarm")</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstanarm)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>fit_rstanarm <span class="ot">&lt;-</span> <span class="fu">stan_glmer</span>(arma_series <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> arma_series), <span class="at">data =</span> <span class="fu">data.frame</span>(arma_series), <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                           <span class="at">sparse =</span> <span class="cn">FALSE</span>, <span class="at">REFORMULATE =</span> <span class="cn">NULL</span>, <span class="at">subset =</span> <span class="cn">NULL</span>, <span class="at">na.action =</span> na.fail,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                           <span class="at">contrasts =</span> <span class="cn">NULL</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.95</span>),</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                           <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">200</span>, <span class="at">quiet =</span> <span class="cn">TRUE</span>, <span class="at">refresh =</span> <span class="dv">0</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_rstanarm)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_rstanarm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li><code>#install.packages("rstanarm")</code>: Installs the <code>rstanarm</code> package for Bayesian modeling in R, if it hasn’t already been installed.</li>
<li><code>library(rstanarm)</code>: Loads the <code>rstanarm</code> package for Bayesian modeling in R.</li>
<li><code>fit_rstanarm &lt;- stan_glmer(...)</code>: Uses the <code>stan_glmer()</code> function to fit the Bayesian ARMA(1, 1) model. The formula passed to the function is <code>arma_series ~ 1 + (1 | arma_series)</code>, indicating that the response variable is <code>arma_series</code>, and we have a simple intercept and a random intercept term <code>(1 | arma_series)</code> included in the model.</li>
<li><code>family = gaussian()</code>: Indicates that the response variable follows a normal distribution.</li>
<li><code>chains = 4, cores = 4, iter = 2000, control = list(adapt_delta = 0.95)</code>: Configures various settings for running the MCMC algorithm, similar to the <code>brms</code> example.</li>
<li><code>summary(fit_rstanarm)</code>: Summarizes the posterior distribution of the parameters in the model.</li>
<li><code>plot(fit_rstanarm)</code>: Plots the posterior distributions of the parameters.</li>
</ol>
</section>
<section id="volatility-models-ex" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="volatility-models-ex"><span class="header-section-number">6.6.3</span> Volatility Models EX</h3>
<p>For volatility models, we cannot directly apply the BRMS or rstanarm packages, so I will present an example of GARCH(1,1) in pure STAN language, which can be executed using the command line interface or RStan package.</p>
<section id="step-1-define-the-dataset" class="level4" data-number="6.6.3.1">
<h4 data-number="6.6.3.1" class="anchored" data-anchor-id="step-1-define-the-dataset"><span class="header-section-number">6.6.3.1</span> Step 1: Define the dataset</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rugarch)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(sp500ret)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>sp500ret <span class="ot">&lt;-</span> sp500ret[,<span class="st">"SP500RET"</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(sp500ret)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li><code>library(rugarch)</code>: Loads the rugarch library, which contains predefined models and utilities for time series analysis.</li>
<li><code>data(sp500ret)</code>: Loads the SP500 dataset from the rugarch library.</li>
<li><code>sp500ret &lt;- sp500ret[,"SP500RET"]</code>: Selects the SP500 RETurn series from the loaded dataset.</li>
<li><code>n &lt;- length(sp500ret)</code>: Gets the length of the time series.</li>
</ol>
</section>
<section id="step-2-write-the-stan-code-for-garch11" class="level4" data-number="6.6.3.2">
<h4 data-number="6.6.3.2" class="anchored" data-anchor-id="step-2-write-the-stan-code-for-garch11"><span class="header-section-number">6.6.3.2</span> Step 2: Write the STAN code for GARCH(1,1)</h4>
<p>Create a file named <code>garch11.stan</code> and paste the following code inside:</p>
<pre><code>data {
  int&lt;lower=0&gt; n; // number of observations
  real y[n]; // input time series
}

parameters {
  real&lt;lower=0&gt; omega; // persistence of shocks
  real&lt;lower=0&gt; alpha1; // short-term shock decay
  real&lt;lower=0&gt; beta1; // long-term shock decay
}

model {
  real mu; // expectation of time series
  real sigma[n]; // residual volatility

  mu &lt;- 0; // time series expectation

  // GARCH(1,1) model definition
  sigma[1] &lt;- sqrt(omega);
  for (i in 2:n) {
    sigma[i] &lt;- sqrt(omega + alpha1 * pow(y[i-1], 2) + beta1 * pow(sigma[i-1], 2));
  }

  // likelihood calculation
  for (i in 1:n) {
    y[i] ~ normal(mu, sigma[i]);
  }
}</code></pre>
<p>This STAN code defines a GARCH(1,1) model with three parameters: <code>omega</code>, <code>alpha1</code>, and <code>beta1</code>. It calculates the likelihood of the observed time series under the proposed GARCH(1,1) model.</p>
</section>
<section id="step-3-fit-the-garch11-model-using-stan" class="level4" data-number="6.6.3.3">
<h4 data-number="6.6.3.3" class="anchored" data-anchor-id="step-3-fit-the-garch11-model-using-stan"><span class="header-section-number">6.6.3.3</span> Step 3: Fit the GARCH(1,1) model using STAN</h4>
<p>First, install and load the RStan package:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages('rstan')</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstan)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Execute the following commands to compile the STAN code and fit the GARCH(1,1) model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>stan_model <span class="ot">&lt;-</span> <span class="fu">stan_model</span>(<span class="st">'garch11.stan'</span>);</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>fit_garch <span class="ot">&lt;-</span> <span class="fu">sampling</span>(stan_model, <span class="at">data =</span> <span class="fu">list</span>(<span class="at">n =</span> n, <span class="at">y =</span> sp500ret));</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-4-diagnose-and-evaluate-the-model" class="level4" data-number="6.6.3.4">
<h4 data-number="6.6.3.4" class="anchored" data-anchor-id="step-4-diagnose-and-evaluate-the-model"><span class="header-section-number">6.6.3.4</span> Step 4: Diagnose and evaluate the model</h4>
<p>Diagnose the model fit using diagnostic plots:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stan_diagnostic_plots</span>(fit_garch);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Extract the posterior samples and analyse the results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>fit_garch_draws <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_garch);</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_garch_draws);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>These explanations accompany the provided codes to help you understand the process of simulating an ARMA(1, 1) process and performing Bayesian estimation using STAN. You can experiment with changing the model parameters and analysing different datasets to deepen your understanding and skills.</p>
<p>Stochastic Volatility (SV) is another popular volatility model in finance. Similar to GARCH, it models the variance of a time series as a latent process affected by shocks. Unlike GARCH, SV treats the variance as a random variable following a continuous-time diffusion process.</p>
<p>In this example, I will show you how to implement a Stochastic Volatility model using the brms package.</p>
</section>
<section id="dataset-preparation" class="level4" data-number="6.6.3.5">
<h4 data-number="6.6.3.5" class="anchored" data-anchor-id="dataset-preparation"><span class="header-section-number">6.6.3.5</span> Dataset Preparation</h4>
<p>Use the same SP500 dataset as before:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rugarch)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(sp500ret)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>sp500ret <span class="ot">&lt;-</span> sp500ret[,<span class="st">"SP500RET"</span>]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(sp500ret)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="implementing-the-stochastic-volatility-model-in-brms" class="level4" data-number="6.6.3.6">
<h4 data-number="6.6.3.6" class="anchored" data-anchor-id="implementing-the-stochastic-volatility-model-in-brms"><span class="header-section-number">6.6.3.6</span> Implementing the Stochastic Volatility Model in brms</h4>
<p>Instead of working directly with the original data, create a transformed dataset with logarithmic volatilities:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>volatilities <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">abs</span>(<span class="fu">diff</span>(sp500ret)))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>volatilities_transformed <span class="ot">&lt;-</span> <span class="fu">log</span>(volatilities)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Transformed dataset is stored in <code>volatilities_transformed</code>. Now, we can fit the SV model using brms:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(brms)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>fit_sv <span class="ot">&lt;-</span> <span class="fu">brm</span>(<span class="fu">bf</span>(volatilities_transformed <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">t</span>(volatilities_transformed),</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                  phi <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">t</span>(volatilities_transformed),</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">nl =</span> <span class="cn">TRUE</span>),</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> <span class="fu">data.frame</span>(volatilities_transformed),</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">200</span>, <span class="at">warmup =</span> <span class="dv">1000</span>,</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.95</span>, <span class="at">max_treedepth =</span> <span class="dv">15</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Formula includes two parts separated by a comma:</p>
<ol type="1">
<li><code>volatilities_transformed ~ 1 + t(volatilities_transformed), phi ~ 1 + t(volatilities_transformed)</code>: Use the transformed volatilities as the response variable and model its dynamics with a first-order auto-regressive component (phi).</li>
<li><code>nl = TRUE</code>: Allow nonlinear optimization for the model.</li>
</ol>
<p>Other arguments configure the MCMC settings.</p>
</section>
<section id="results-analysis" class="level4" data-number="6.6.3.7">
<h4 data-number="6.6.3.7" class="anchored" data-anchor-id="results-analysis"><span class="header-section-number">6.6.3.7</span> Results Analysis</h4>
<p>analyse the results similarly to the GARCH example:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stan_diagnostic_plots</span>(fit_sv)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_sv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This example demonstrates how to implement a Stochastic Volatility model using brms. You can modify the model settings or change the dataset to develop a deeper understanding and skillset.</p>
</section>
</section>
</section>
<section id="common-misconceptions-of-bayesian-econometrics-by-traditional-frequentist-econometricians" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="common-misconceptions-of-bayesian-econometrics-by-traditional-frequentist-econometricians"><span class="header-section-number">6.7</span> Common Misconceptions of Bayesian Econometrics by Traditional Frequentist Econometricians</h2>
<p>The evolution of statistical methods in econometrics has seen the Bayesian approach increasingly gaining acceptance alongside the classical frequentist methods. Historically, there were significant debates between these two schools of thought, each with its own advocates and critics. While these approaches are now both widely accepted, some common misconceptions about Bayesian econometrics persist among traditional frequentist econometricians:</p>
<ol type="1">
<li><p><strong>Prior Beliefs Over Emphasis</strong>: A common misconception is that Bayesian analysis overly relies on subjective prior beliefs, potentially skewing the results. However, Bayesian methods systematically update these beliefs with objective data, balancing prior knowledge with empirical evidence.</p></li>
<li><p><strong>Complexity and Intractability</strong>: There is a notion that Bayesian methods are inherently more complex and less tractable than frequentist methods. Advances in computational techniques, particularly Markov Chain Monte Carlo (MCMC) methods, have greatly improved the feasibility and practicality of Bayesian analysis, making it more accessible.</p></li>
<li><p><strong>Lack of Objectivity</strong>: Some frequentists perceive Bayesian methods as less objective due to the incorporation of prior beliefs. In reality, Bayesian inference provides a framework that integrates prior information with data, leading to a comprehensive understanding of the uncertainty in model parameters and predictions.</p></li>
<li><p><strong>Inferential Differences</strong>: There’s a belief that Bayesian and frequentist methods lead to fundamentally different inferences. While approaches may differ in methodology, they often yield similar results, with Bayesian solutions sometimes offering advantages, especially in cases with limited data or complex models.</p></li>
<li><p><strong>Bayesian Methods as a Last Resort</strong>: Another misconception is that Bayesian methods are only used when frequentist methods fail. In contrast, Bayesian analysis has its own strengths and is often used as a primary approach in many scenarios in finance and econometrics.</p></li>
<li><p><strong>Inapplicability in Certain Areas</strong>: It’s sometimes thought that Bayesian methods are not applicable to certain problems in econometrics. However, with the advancement in computational methods, Bayesian solutions have been developed for a broad range of problems, often providing insightful and useful results.</p></li>
</ol>
<p>The shift in perception and understanding of Bayesian methods highlights the growing recognition of their value in econometric analysis. As computational capabilities continue to evolve, the practicality and applicability of Bayesian methods in finance and econometrics are likely to expand even further</p>
<section id="further-reading-for-bayesian-methods-in-finance" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="further-reading-for-bayesian-methods-in-finance"><span class="header-section-number">6.7.1</span> Further Reading for Bayesian Methods in Finance</h3>
<p>Certainly! Here are links to the recommended further reading books on Bayesian methods in finance. These links typically lead to the books’ pages on Amazon or the publishers’ websites, where you can find more details:</p>
<ol type="1">
<li><strong>“Bayesian Data Analysis” by Andrew Gelman et al.</strong>
<ul>
<li><a href="https://www.amazon.com/Bayesian-Data-Analysis-Third-Chapman/dp/1439840954">Amazon Link</a></li>
</ul></li>
<li><strong>“The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation” by Christian P. Robert</strong>
<ul>
<li><a href="https://www.amazon.com/Bayesian-Choice-Decision-Theoretic-Computational-Implementation/dp/0387715983">Amazon Link</a></li>
</ul></li>
<li><strong>“Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan” by John Kruschke</strong>
<ul>
<li><a href="https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884">Amazon Link</a></li>
</ul></li>
<li><strong>“Bayesian Analysis with Python” by Osvaldo Martin</strong>
<ul>
<li><a href="https://www.amazon.com/Bayesian-Analysis-Python-Osvaldo-Martin/dp/1789341655">Amazon Link</a></li>
</ul></li>
<li><strong>“Statistical Rethinking: A Bayesian Course with Examples in R and Stan” by Richard McElreath</strong>
<ul>
<li><a href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/036713991X">Amazon Link</a></li>
</ul></li>
<li><strong>“Bayesian Methods for Statistical Analysis” by Borek Puza</strong>
<ul>
<li><a href="https://www.amazon.com/Bayesian-Methods-Statistical-Analysis-Puza/dp/0646545093">Amazon Link</a></li>
</ul></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(scopus.R)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Brooks2019" class="csl-entry" role="listitem">
Brooks, Christopher M. 2019. <em>Introductory Econometrics for Finance</em>. Cambridge University Press.
</div>
<div id="ref-Jennifer2021" class="csl-entry" role="listitem">
Brown, Pulak;Gray, Sarah;Ghosh. 2021. <span>“Saving Behaviour and Health: A High-Dimensional Bayesian Analysis of British Panel Data.”</span> <em>European Journal of Finance</em> 27: 1581–1603. <a href="https://doi.org/10.1080/1351847X.2021.1899953">https://doi.org/10.1080/1351847X.2021.1899953</a>.
</div>
<div id="ref-Cremers2018" class="csl-entry" role="listitem">
Brühl, Kristina, Alexander Engelberg, Ralph Koijen, and Stephan Siegel. 2018. <span>“Artificial Intelligence in Finance.”</span> <em>Review of Corporate Finance Studies</em> 7 (1): 1–32. <a href="https://doi.org/10.1093/rcfs/cfw034">https://doi.org/10.1093/rcfs/cfw034</a>.
</div>
<div id="ref-Lingxiao2020" class="csl-entry" role="listitem">
Chib, Xiaming;Zhao, Siddhartha;Zeng. 2020. <span>“On Comparing Asset Pricing Models.”</span> <em>Journal of Finance</em> 75: 551–77. <a href="https://doi.org/10.1111/jofi.12854">https://doi.org/10.1111/jofi.12854</a>.
</div>
<div id="ref-Diebold2015" class="csl-entry" role="listitem">
Diebold, Francis X. 2015. <em>Elements of Forecasting</em>. Thomson Higher Education.
</div>
<div id="ref-DAVID1989" class="csl-entry" role="listitem">
FELDMAN, DAVID. 1989. <span>“The Term Structure of Interest Rates in a Partially Observable Economy.”</span> <em>The Journal of Finance</em> 44: 789–812. <a href="https://doi.org/10.1111/j.1540-6261.1989.tb04391.x">https://doi.org/10.1111/j.1540-6261.1989.tb04391.x</a>.
</div>
<div id="ref-Gelman2013" class="csl-entry" role="listitem">
Gelman, Andrew, Jennifer Hill, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. 2013. <em>Bayesian Data Analysis</em>. Chapman; Hall/CRC.
</div>
<div id="ref-Gelman2020" class="csl-entry" role="listitem">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. <span>“<span class="nocase">Regression and Other Stories</span>.”</span> <a href="https://doi.org/10.1017/9781139161879">https://doi.org/10.1017/9781139161879</a>.
</div>
<div id="ref-Greene2018" class="csl-entry" role="listitem">
Greene, William H. 2018. <em>Econometric Analysis</em>. Pearson Education Limited.
</div>
<div id="ref-Andrey2010" class="csl-entry" role="listitem">
Grenadier, Andrey, Steven R.;Malenko. 2010. <span>“A Bayesian Approach to Real Options: The Case of Distinguishing Between Temporary and Permanent Shocks.”</span> <em>Journal of Finance</em> 65: 1949–86. <a href="https://doi.org/10.1111/j.1540-6261.2010.01599.x">https://doi.org/10.1111/j.1540-6261.2010.01599.x</a>.
</div>
<div id="ref-Hamilton1994" class="csl-entry" role="listitem">
Hamilton, James Douglas. 1994. <em>Time Series Analysis</em>. Princeton University Press.
</div>
<div id="ref-Hartmann2013" class="csl-entry" role="listitem">
Hartmann, Stefan, and Wolfgang Hardle. 2013. <em>Applied Multivariate Time Series Analysis: State Space and Kalman Filter Approach</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-Chi2022" class="csl-entry" role="listitem">
Huang, Mei Chi. 2022. <span>“Time-Varying Roles of Housing Risk Factors in State-Level Housing Markets.”</span> <em>International Journal of Finance and Economics</em> 27: 4660–83. <a href="https://doi.org/10.1002/ijfe.2393">https://doi.org/10.1002/ijfe.2393</a>.
</div>
<div id="ref-Heje2023" class="csl-entry" role="listitem">
Jensen, Bryan;Pedersen, Theis Ingerslev;Kelly. 2023. <span>“Is There a Replication Crisis in Finance?”</span> <em>Journal of Finance</em> 78: 2465–2518. <a href="https://doi.org/10.1111/jofi.13249">https://doi.org/10.1111/jofi.13249</a>.
</div>
<div id="ref-Jonah2019" class="csl-entry" role="listitem">
Jonah, Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. <span>“<span class="nocase">Visualization in Bayesian workflow</span>.”</span> <em>J. R. Stat. Soc. Ser. A Stat. Soc.</em> 182 (2): 389–402. <a href="https://doi.org/10.1111/rssa.12378">https://doi.org/10.1111/rssa.12378</a>.
</div>
<div id="ref-O.1975" class="csl-entry" role="listitem">
Joy, John O., O. Maurice;Tollefson. 1975. <span>“On the Financial Applications of Discriminant Analysis.”</span> <em>Journal of Financial and Quantitative Analysis</em> 10: 723–39. <a href="https://doi.org/10.2307/2330267">https://doi.org/10.2307/2330267</a>.
</div>
<div id="ref-Samuli2008" class="csl-entry" role="listitem">
Kaustia, Samuli, Markku;Knüpfer. 2008. <span>“Do Investors Overweight Personal Experience? Evidence from IPO Subscriptions.”</span> <em>Journal of Finance</em> 63: 2679–2702. <a href="https://doi.org/10.1111/j.1540-6261.2008.01411.x">https://doi.org/10.1111/j.1540-6261.2008.01411.x</a>.
</div>
<div id="ref-Kelly2022complexity" class="csl-entry" role="listitem">
Kelly, Bryan T, Semyon Malamud, and Kangying Zhou. 2022. <span>“<span class="nocase">The Virtue of Complexity Everywhere</span>.”</span> <em>SSRN Electronic Journal</em>. <a href="https://doi.org/10.2139/ssrn.4171581">https://doi.org/10.2139/ssrn.4171581</a>.
</div>
<div id="ref-Kelly2023FML" class="csl-entry" role="listitem">
Kelly, Bryan T., and Dacheng Xiu. 2023. <span>“<span>Financial Machine Learning</span>.”</span> <em>SSRN Electronic Journal</em>. <a href="https://doi.org/10.2139/ssrn.4520856">https://doi.org/10.2139/ssrn.4520856</a>.
</div>
<div id="ref-Hui2009" class="csl-entry" role="listitem">
Li, Hui, C. Wei;Xue. 2009. <span>“A Bayesian’s Bubble.”</span> <em>Journal of Finance</em> 64: 2665–2701. <a href="https://doi.org/10.1111/j.1540-6261.2009.01514.x">https://doi.org/10.1111/j.1540-6261.2009.01514.x</a>.
</div>
<div id="ref-lo1997econometrics" class="csl-entry" role="listitem">
Lo, Andrew W., and A. Craig MacKinlay. 1997. <em>The Econometrics of Financial Markets</em>. Princeton University Press.
</div>
<div id="ref-Ang2002" class="csl-entry" role="listitem">
Lo, Andrew, and Craig MacKinlay. 2002. <em>Analysis of Financial Time Series</em>. Oxford University Press. <a href="https://doi.org/10.1093/he/9780198776697.001.0001">https://doi.org/10.1093/he/9780198776697.001.0001</a>.
</div>
<div id="ref-Murphy2012" class="csl-entry" role="listitem">
Murphy, Kevin P. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press.
</div>
<div id="ref-S.2015" class="csl-entry" role="listitem">
Paolella, Marc S. 2015. <span>“Multivariate Asset Return Prediction with Mixture Models.”</span> <em>European Journal of Finance</em> 21: 1214–52. <a href="https://doi.org/10.1080/1351847X.2012.760167">https://doi.org/10.1080/1351847X.2012.760167</a>.
</div>
<div id="ref-Ľuboš2000" class="csl-entry" role="listitem">
Pástor, Ľuboš. 2000. <span>“Portfolio Selection and Asset Pricing Models.”</span> <em>Journal of Finance</em> 55: 179–223. <a href="https://doi.org/10.1111/0022-1082.00204">https://doi.org/10.1111/0022-1082.00204</a>.
</div>
<div id="ref-Peter2015" class="csl-entry" role="listitem">
Payzan-Lenestour, Peter, Elise;Bossaerts. 2015. <span>“Learning about Unstable, Publicly Unobservable Payoffs.”</span> <em>Review of Financial Studies</em> 28: 1874–1913. <a href="https://doi.org/10.1093/rfs/hhu069">https://doi.org/10.1093/rfs/hhu069</a>.
</div>
<div id="ref-BRYAN1986" class="csl-entry" role="listitem">
STANHOUSE, BRYAN. 1986. <span>“Commercial Bank Portfolio Behavior and Endogenous Uncertainty.”</span> <em>The Journal of Finance</em> 41: 1103–14. <a href="https://doi.org/10.1111/j.1540-6261.1986.tb02533.x">https://doi.org/10.1111/j.1540-6261.1986.tb02533.x</a>.
</div>
<div id="ref-LARRY1979" class="csl-entry" role="listitem">
STANHOUSE, LARRY, BRYAN;SHERMAN. 1979. <span>“A Note on Information in the Loan Evaluation Process.”</span> <em>The Journal of Finance</em> 34: 1263–69. <a href="https://doi.org/10.1111/j.1540-6261.1979.tb00072.x">https://doi.org/10.1111/j.1540-6261.1979.tb00072.x</a>.
</div>
<div id="ref-Long2017" class="csl-entry" role="listitem">
Stoughton, Kit Pong;Yi, Neal M.;Wong. 2017. <span>“Investment Efficiency and Product Market Competition.”</span> <em>Journal of Financial and Quantitative Analysis</em> 52: 2611–42. <a href="https://doi.org/10.1017/S0022109017000746">https://doi.org/10.1017/S0022109017000746</a>.
</div>
<div id="ref-Tsay2014" class="csl-entry" role="listitem">
Tsay, Ruey S. 2014. <em>Multivariate Time Series Analysis: With r and Financial Applications</em>. John Wiley &amp; Sons, Ltd.
</div>
<div id="ref-Ionut2009" class="csl-entry" role="listitem">
Ulibarri, Peter C.;Hovsepian, Carlos A.;Anselmo. 2009. <span>“Erratum: ’Noise-Trader Risk’ and Bayesian Market Making in FX Derivatives: Rolling Loaded Dice? (International Journal of Finance and Economics (2008)).”</span> <em>International Journal of Finance and Economics</em> 14: N/A. <a href="https://doi.org/10.1002/ijfe.388">https://doi.org/10.1002/ijfe.388</a>.
</div>
<div id="ref-Vilhuber2021" class="csl-entry" role="listitem">
Vilhuber, Lars. 2021. <span>“Reproducibility and Replicability in Economics.”</span> <em>Annual Review of Economics</em> 13 (1): 45–70.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./time_series.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Financial times series econometrics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ml.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Machine Learning in Finance</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>